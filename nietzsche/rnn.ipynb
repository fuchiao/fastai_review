{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# char2indices, indices2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('nietzsche.txt').read()\n",
    "chars = sorted(list(set(text)))\n",
    "chars.insert(0, \"\\0\")  # for padding\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = [char_indices[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cs=3\n",
    "c1_dat = [idx[i] for i in range(0, len(idx)-1-cs, cs)]\n",
    "c2_dat = [idx[i+1] for i in range(0, len(idx)-1-cs, cs)]\n",
    "c3_dat = [idx[i+2] for i in range(0, len(idx)-1-cs, cs)]\n",
    "c4_dat = [idx[i+3] for i in range(0, len(idx)-1-cs, cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x1 = np.stack(c1_dat[:-2])\n",
    "x2 = np.stack(c2_dat[:-2])\n",
    "x3 = np.stack(c3_dat[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.stack(c4_dat[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.merge import add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_input(name, n_in, n_out):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    emb = Embedding(n_in, n_out, input_length=1)(inp)\n",
    "    return inp, Flatten()(emb)\n",
    "n_fac = 42\n",
    "c1_in, c1 = embedding_input('c1', vocab_size, n_fac)\n",
    "c2_in, c2 = embedding_input('c2', vocab_size, n_fac)\n",
    "c3_in, c3 = embedding_input('c3', vocab_size, n_fac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_in = Dense(256, activation='relu')\n",
    "dense_hidden = Dense(256, activation='relu')\n",
    "\n",
    "c1_hidden = dense_in(c1)\n",
    "\n",
    "c2_dense = dense_in(c2)\n",
    "hidden_2 = dense_hidden(c1_hidden)\n",
    "c2_hidden = add([c2_dense, hidden_2])\n",
    "\n",
    "c3_dense = dense_in(c3)\n",
    "hidden_3 = dense_hidden(c2_hidden)\n",
    "c3_hidden = add([c3_dense, hidden_3])\n",
    "\n",
    "dense_out = Dense(vocab_size, activation='softmax')\n",
    "c4_out = dense_out(c3_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "c3 (InputLayer)                  (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c2 (InputLayer)                  (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c1 (InputLayer)                  (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 1, 42)         3570        c3[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 1, 42)         3570        c2[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 1, 42)         3570        c1[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 42)            0           embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 42)            0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 42)            0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 256)           11008       flatten_1[0][0]                  \n",
      "                                                                   flatten_2[0][0]                  \n",
      "                                                                   flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 256)           65792       dense_9[0][0]                    \n",
      "                                                                   add_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_5 (Add)                      (None, 256)           0           dense_9[1][0]                    \n",
      "                                                                   dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "add_6 (Add)                      (None, 256)           0           dense_9[2][0]                    \n",
      "                                                                   dense_10[1][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 85)            21845       add_6[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 109,355\n",
      "Trainable params: 109,355\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([c1_in, c2_in, c3_in], c4_out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "200295/200295 [==============================] - 18s - loss: 2.3414    \n",
      "Epoch 2/4\n",
      "200295/200295 [==============================] - 18s - loss: 2.1524    \n",
      "Epoch 3/4\n",
      "200295/200295 [==============================] - 18s - loss: 2.1040    \n",
      "Epoch 4/4\n",
      "200295/200295 [==============================] - 18s - loss: 2.0810    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb44e41acc0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.fit([x1, x2, x3], y, batch_size=64, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_next(inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arrs = [np.array([i]) for i in idxs]\n",
    "    p = model.predict(arrs)\n",
    "    i = np.argmax(p)\n",
    "    return chars[i]\n",
    "get_next(' an')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(' th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('phi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cs=8\n",
    "c_in_dat = [[idx[i+n] for i in range(0, len(idx)-1-cs, cs)] for n in range(cs)]\n",
    "c_out_dat = [idx[i+cs] for i in range(0, len(idx)-1-cs, cs)]\n",
    "xs = [np.stack(c[:-2]) for c in c_in_dat]\n",
    "y = np.stack(c_out_dat[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([40,  1, 33,  2, 72, 67, 73,  2]),\n",
       " array([42,  1, 38, 44,  2,  9, 61, 73]),\n",
       " array([29, 43, 31, 71, 54,  9, 58, 61]),\n",
       " array([30, 45,  2, 74,  2, 76, 67, 58]),\n",
       " array([25, 40, 73, 73, 76, 61, 24, 71]),\n",
       " array([27, 40, 61, 61, 68, 54,  2, 58]),\n",
       " array([29, 39, 54,  2, 66, 73, 33,  2]),\n",
       " array([ 1, 43, 73, 62, 54,  2, 72, 67])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[xs[n][:cs] for n in range(cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 33,  2, 72, 67, 73,  2, 68])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:cs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_input(name, n_in, n_out):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name+'_in')\n",
    "    emb = Embedding(n_in, n_out, input_length=1, name=name+'_emb')(inp)\n",
    "    return inp, Flatten()(emb)\n",
    "n_fac = 42\n",
    "c_ins = [embedding_input('c'+str(n), vocab_size, n_fac) for n in range(cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_in = Dense(256, activation='relu')\n",
    "dense_hidden = Dense(256, activation='relu', kernel_initializer='identity')\n",
    "dense_out = Dense(vocab_size, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = dense_in(c_ins[0][1])   # c_ins[0][1] = flattened embedding of the first character\n",
    "for i in range(1,cs):\n",
    "    c_dense = dense_in(c_ins[i][1])\n",
    "    hidden = dense_hidden(hidden)\n",
    "    hidden = add([c_dense, hidden])\n",
    "c_out = dense_out(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model([c[0] for c in c_ins], c_out)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "75109/75109 [==============================] - 11s - loss: 2.5333    \n",
      "Epoch 2/12\n",
      "75109/75109 [==============================] - 11s - loss: 2.2549    \n",
      "Epoch 3/12\n",
      "75109/75109 [==============================] - 11s - loss: 2.1539    \n",
      "Epoch 4/12\n",
      "75109/75109 [==============================] - 11s - loss: 2.0848    \n",
      "Epoch 5/12\n",
      "75109/75109 [==============================] - 11s - loss: 2.0285    \n",
      "Epoch 6/12\n",
      "75109/75109 [==============================] - 11s - loss: 1.9816    \n",
      "Epoch 7/12\n",
      "75109/75109 [==============================] - 11s - loss: 1.9401    \n",
      "Epoch 8/12\n",
      "75109/75109 [==============================] - 11s - loss: 1.9038    \n",
      "Epoch 9/12\n",
      "75109/75109 [==============================] - 11s - loss: 1.8725    \n",
      "Epoch 10/12\n",
      "75109/75109 [==============================] - 11s - loss: 1.8430    \n",
      "Epoch 11/12\n",
      "75109/75109 [==============================] - 11s - loss: 1.8157    \n",
      "Epoch 12/12\n",
      "75109/75109 [==============================] - 11s - loss: 1.7926    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb40b74fda0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xs, y, batch_size=64, epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = [np.array(char_indices[c])[np.newaxis] for c in inp]\n",
    "    p = model.predict(idxs)\n",
    "    return chars[np.argmax(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('part of ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('queens a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 8, 85)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_fac, cs, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 8, 42)             3570      \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, 256)               76544     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 85)                21845     \n",
      "=================================================================\n",
      "Total params: 101,959\n",
      "Trainable params: 101,959\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=cs),\n",
    "        SimpleRNN(256, activation='relu', recurrent_initializer='identity'),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "75109/75109 [==============================] - 7s - loss: 2.8040     \n",
      "Epoch 2/8\n",
      "75109/75109 [==============================] - 7s - loss: 2.2879     \n",
      "Epoch 3/8\n",
      "75109/75109 [==============================] - 7s - loss: 2.0780     \n",
      "Epoch 4/8\n",
      "75109/75109 [==============================] - 7s - loss: 1.9352     \n",
      "Epoch 5/8\n",
      "75109/75109 [==============================] - 7s - loss: 1.8298     \n",
      "Epoch 6/8\n",
      "75109/75109 [==============================] - 7s - loss: 1.7470     \n",
      "Epoch 7/8\n",
      "75109/75109 [==============================] - 7s - loss: 1.6833     \n",
      "Epoch 8/8\n",
      "75109/75109 [==============================] - 7s - loss: 1.6292     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb40a928fd0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.fit(np.concatenate(xs,axis=1), y, batch_size=64, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_keras(inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arrs = np.array(idxs)[np.newaxis,:]\n",
    "    p = model.predict(arrs)[0]\n",
    "    return chars[np.argmax(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_keras('this is ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_keras('part of ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_keras('queens a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# returning sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_in_dat = [[idx[i+n] for i in range(0, len(idx)-1-cs, cs)] for n in range(cs)]\n",
    "c_out_dat = [[idx[i+n] for i in range(1, len(idx)-cs, cs)] for n in range(cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([42,  1, 38, 44,  2,  9, 61, 73]),\n",
       " array([29, 43, 31, 71, 54,  9, 58, 61]),\n",
       " array([30, 45,  2, 74,  2, 76, 67, 58]),\n",
       " array([25, 40, 73, 73, 76, 61, 24, 71]),\n",
       " array([27, 40, 61, 61, 68, 54,  2, 58]),\n",
       " array([29, 39, 54,  2, 66, 73, 33,  2]),\n",
       " array([ 1, 43, 73, 62, 54,  2, 72, 67]),\n",
       " array([ 1, 33,  2, 72, 67, 73,  2, 68])]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys = [np.stack(c[:-2]) for c in c_out_dat]\n",
    "[ys[n][:cs] for n in range(cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([40,  1, 33,  2, 72, 67, 73,  2]),\n",
       " array([42,  1, 38, 44,  2,  9, 61, 73]),\n",
       " array([29, 43, 31, 71, 54,  9, 58, 61]),\n",
       " array([30, 45,  2, 74,  2, 76, 67, 58]),\n",
       " array([25, 40, 73, 73, 76, 61, 24, 71]),\n",
       " array([27, 40, 61, 61, 68, 54,  2, 58]),\n",
       " array([29, 39, 54,  2, 66, 73, 33,  2]),\n",
       " array([ 1, 43, 73, 62, 54,  2, 72, 67])]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = [np.stack(c[:-2]) for c in c_in_dat]\n",
    "[xs[n][:cs] for n in range(cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_in = Dense(n_hidden, activation='relu')\n",
    "dense_hidden = Dense(n_hidden, activation='relu', kernel_initializer='identity')\n",
    "dense_out = Dense(vocab_size, activation='softmax', name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "c0_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c0_emb (Embedding)               (None, 1, 42)         3570        c0_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 42)            0           c0_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "zeros (InputLayer)               (None, 42)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (None, 256)           11008       zeros[0][0]                      \n",
      "                                                                   flatten_4[0][0]                  \n",
      "                                                                   flatten_5[0][0]                  \n",
      "                                                                   flatten_6[0][0]                  \n",
      "                                                                   flatten_7[0][0]                  \n",
      "                                                                   flatten_8[0][0]                  \n",
      "                                                                   flatten_9[0][0]                  \n",
      "                                                                   flatten_10[0][0]                 \n",
      "                                                                   flatten_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "c1_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (None, 256)           65792       dense_24[0][0]                   \n",
      "                                                                   add_14[0][0]                     \n",
      "                                                                   add_15[0][0]                     \n",
      "                                                                   add_16[0][0]                     \n",
      "                                                                   add_17[0][0]                     \n",
      "                                                                   add_18[0][0]                     \n",
      "                                                                   add_19[0][0]                     \n",
      "                                                                   add_20[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c1_emb (Embedding)               (None, 1, 42)         3570        c1_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_14 (Add)                     (None, 256)           0           dense_24[1][0]                   \n",
      "                                                                   dense_25[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 42)            0           c1_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c2_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c2_emb (Embedding)               (None, 1, 42)         3570        c2_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_15 (Add)                     (None, 256)           0           dense_24[2][0]                   \n",
      "                                                                   dense_25[1][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 42)            0           c2_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c3_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c3_emb (Embedding)               (None, 1, 42)         3570        c3_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_16 (Add)                     (None, 256)           0           dense_24[3][0]                   \n",
      "                                                                   dense_25[2][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)              (None, 42)            0           c3_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c4_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c4_emb (Embedding)               (None, 1, 42)         3570        c4_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_17 (Add)                     (None, 256)           0           dense_24[4][0]                   \n",
      "                                                                   dense_25[3][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)              (None, 42)            0           c4_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c5_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c5_emb (Embedding)               (None, 1, 42)         3570        c5_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_18 (Add)                     (None, 256)           0           dense_24[5][0]                   \n",
      "                                                                   dense_25[4][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)              (None, 42)            0           c5_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c6_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c6_emb (Embedding)               (None, 1, 42)         3570        c6_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_19 (Add)                     (None, 256)           0           dense_24[6][0]                   \n",
      "                                                                   dense_25[5][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)             (None, 42)            0           c6_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c7_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c7_emb (Embedding)               (None, 1, 42)         3570        c7_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_20 (Add)                     (None, 256)           0           dense_24[7][0]                   \n",
      "                                                                   dense_25[6][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)             (None, 42)            0           c7_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "add_21 (Add)                     (None, 256)           0           dense_24[8][0]                   \n",
      "                                                                   dense_25[7][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 85)            21845       add_14[0][0]                     \n",
      "                                                                   add_15[0][0]                     \n",
      "                                                                   add_16[0][0]                     \n",
      "                                                                   add_17[0][0]                     \n",
      "                                                                   add_18[0][0]                     \n",
      "                                                                   add_19[0][0]                     \n",
      "                                                                   add_20[0][0]                     \n",
      "                                                                   add_21[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 127,205\n",
      "Trainable params: 127,205\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp1 = Input(shape=(n_fac,), name='zeros')\n",
    "hidden = dense_in(inp1)\n",
    "\n",
    "outs = []\n",
    "\n",
    "for i in range(cs):\n",
    "    c_dense = dense_in(c_ins[i][1])\n",
    "    hidden = dense_hidden(hidden)\n",
    "    hidden = add([c_dense, hidden])\n",
    "    # every layer now has an output\n",
    "    outs.append(dense_out(hidden))\n",
    "\n",
    "model = Model([inp1] + [c[0] for c in c_ins], outs)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75109, 42)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros = np.tile(np.zeros(n_fac), (len(xs[0]), 1))\n",
    "zeros.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "75109/75109 [==============================] - 25s - loss: 20.1408 - output_loss_1: 2.7147 - output_loss_2: 2.5716 - output_loss_3: 2.5115 - output_loss_4: 2.4835 - output_loss_5: 2.4725 - output_loss_6: 2.4655 - output_loss_7: 2.4673 - output_loss_8: 2.4541    \n",
      "Epoch 2/12\n",
      "75109/75109 [==============================] - 24s - loss: 17.8837 - output_loss_1: 2.5155 - output_loss_2: 2.3568 - output_loss_3: 2.2308 - output_loss_4: 2.1792 - output_loss_5: 2.1552 - output_loss_6: 2.1516 - output_loss_7: 2.1605 - output_loss_8: 2.1341    \n",
      "Epoch 3/12\n",
      "75109/75109 [==============================] - 24s - loss: 17.2578 - output_loss_1: 2.4985 - output_loss_2: 2.3338 - output_loss_3: 2.1687 - output_loss_4: 2.0872 - output_loss_5: 2.0506 - output_loss_6: 2.0423 - output_loss_7: 2.0488 - output_loss_8: 2.0279    \n",
      "Epoch 4/12\n",
      "75109/75109 [==============================] - 24s - loss: 16.8847 - output_loss_1: 2.4914 - output_loss_2: 2.3243 - output_loss_3: 2.1361 - output_loss_4: 2.0360 - output_loss_5: 1.9879 - output_loss_6: 1.9734 - output_loss_7: 1.9792 - output_loss_8: 1.9564    \n",
      "Epoch 5/12\n",
      "75109/75109 [==============================] - 24s - loss: 16.6312 - output_loss_1: 2.4875 - output_loss_2: 2.3190 - output_loss_3: 2.1179 - output_loss_4: 2.0003 - output_loss_5: 1.9445 - output_loss_6: 1.9263 - output_loss_7: 1.9286 - output_loss_8: 1.9072     ETA: 3s - loss: 16.6370 - output_loss_1: 2.4890 - output_loss_2: 2.3185 - output_loss_3: \n",
      "Epoch 6/12\n",
      "75109/75109 [==============================] - 24s - loss: 16.4452 - output_loss_1: 2.4851 - output_loss_2: 2.3152 - output_loss_3: 2.1062 - output_loss_4: 1.9739 - output_loss_5: 1.9130 - output_loss_6: 1.8910 - output_loss_7: 1.8909 - output_loss_8: 1.8698    \n",
      "Epoch 7/12\n",
      "75109/75109 [==============================] - 24s - loss: 16.3000 - output_loss_1: 2.4836 - output_loss_2: 2.3119 - output_loss_3: 2.0968 - output_loss_4: 1.9555 - output_loss_5: 1.8886 - output_loss_6: 1.8616 - output_loss_7: 1.8614 - output_loss_8: 1.8405    \n",
      "Epoch 8/12\n",
      "75109/75109 [==============================] - 24s - loss: 16.1873 - output_loss_1: 2.4821 - output_loss_2: 2.3114 - output_loss_3: 2.0886 - output_loss_4: 1.9428 - output_loss_5: 1.8681 - output_loss_6: 1.8387 - output_loss_7: 1.8401 - output_loss_8: 1.8153    \n",
      "Epoch 9/12\n",
      "75109/75109 [==============================] - 24s - loss: 16.0939 - output_loss_1: 2.4809 - output_loss_2: 2.3088 - output_loss_3: 2.0839 - output_loss_4: 1.9306 - output_loss_5: 1.8522 - output_loss_6: 1.8208 - output_loss_7: 1.8205 - output_loss_8: 1.7963    \n",
      "Epoch 10/12\n",
      "75109/75109 [==============================] - 24s - loss: 16.0160 - output_loss_1: 2.4797 - output_loss_2: 2.3085 - output_loss_3: 2.0777 - output_loss_4: 1.9224 - output_loss_5: 1.8397 - output_loss_6: 1.8048 - output_loss_7: 1.8041 - output_loss_8: 1.7790    \n",
      "Epoch 11/12\n",
      "75109/75109 [==============================] - 24s - loss: 15.9507 - output_loss_1: 2.4796 - output_loss_2: 2.3050 - output_loss_3: 2.0759 - output_loss_4: 1.9121 - output_loss_5: 1.8299 - output_loss_6: 1.7924 - output_loss_7: 1.7894 - output_loss_8: 1.7664    \n",
      "Epoch 12/12\n",
      "75109/75109 [==============================] - 24s - loss: 15.8953 - output_loss_1: 2.4782 - output_loss_2: 2.3045 - output_loss_3: 2.0724 - output_loss_4: 1.9064 - output_loss_5: 1.8198 - output_loss_6: 1.7801 - output_loss_7: 1.7789 - output_loss_8: 1.7551    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb409816b38>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([zeros]+xs, ys, batch_size=64, epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nexts(inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arrs = [np.array(i)[np.newaxis] for i in idxs]\n",
    "    p = model.predict([np.zeros(n_fac)[np.newaxis,:]] + arrs)\n",
    "    print(list(inp))\n",
    "    return [chars[np.argmax(o)] for o in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 't', 'h', 'i', 's', ' ', 'i', 's']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a', 'h', 'e', 't', ' ', 's', 'n', ' ']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts(' this is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'p', 'a', 'r', 't', ' ', 'o', 'f']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a', 'o', 'r', 't', 'i', 'o', 'f', ' ']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts(' part of')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sequence model with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.wrappers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 8, 42)             3570      \n",
      "_________________________________________________________________\n",
      "simple_rnn_7 (SimpleRNN)     (None, 8, 256)            76544     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 8, 85)             21845     \n",
      "=================================================================\n",
      "Total params: 101,959\n",
      "Trainable params: 101,959\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=cs),\n",
    "        SimpleRNN(n_hidden, return_sequences=True, activation='relu', recurrent_initializer='identity'),\n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax')),\n",
    "    ])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_rnn=np.stack(np.squeeze(xs), axis=1)\n",
    "y_rnn=np.atleast_3d(np.stack(ys, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75109, 8), (75109, 8, 1))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_rnn.shape, y_rnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "75109/75109 [==============================] - 7s - loss: 2.4342     \n",
      "Epoch 2/8\n",
      "75109/75109 [==============================] - 7s - loss: 1.9967     \n",
      "Epoch 3/8\n",
      "75109/75109 [==============================] - 7s - loss: 1.8817     \n",
      "Epoch 4/8\n",
      "75109/75109 [==============================] - 7s - loss: 1.8217     \n",
      "Epoch 5/8\n",
      "75109/75109 [==============================] - 7s - loss: 1.7840     \n",
      "Epoch 6/8\n",
      "75109/75109 [==============================] - 7s - loss: 1.7571     \n",
      "Epoch 7/8\n",
      "75109/75109 [==============================] - 7s - loss: 1.7378     \n",
      "Epoch 8/8\n",
      "75109/75109 [==============================] - 7s - loss: 1.7213     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb408fe4da0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_rnn, y_rnn, batch_size=64, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 't', 'h', 'i', 's', ' ', 'i', 's']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['t', 'h', 'e', 's', ' ', 's', 's', ' ']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_nexts_keras(inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arr = np.array(idxs)[np.newaxis,:]\n",
    "    p = model.predict(arr)[0]\n",
    "    print(list(inp))\n",
    "    return [chars[np.argmax(o)] for o in p]\n",
    "get_nexts_keras(' this is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stateful model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (64, 8, 42)               3570      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (64, 8, 42)               168       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (64, 8, 256)              306176    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (64, 8, 85)               21845     \n",
      "=================================================================\n",
      "Total params: 331,759\n",
      "Trainable params: 331,675\n",
      "Non-trainable params: 84\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "model=Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=cs, batch_input_shape=(bs,8)),\n",
    "        BatchNormalization(),\n",
    "        LSTM(n_hidden, return_sequences=True, stateful=True),\n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax')),\n",
    "    ])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mx = len(x_rnn)//bs*bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "75072/75072 [==============================] - 27s - loss: 1.7564    \n",
      "Epoch 2/4\n",
      "75072/75072 [==============================] - 26s - loss: 1.7421    \n",
      "Epoch 3/4\n",
      "75072/75072 [==============================] - 26s - loss: 1.7291    - ETA: 0s - l\n",
      "Epoch 4/4\n",
      "75072/75072 [==============================] - 26s - loss: 1.7171    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb40921c588>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_rnn[:mx], y_rnn[:mx], batch_size=bs, epochs=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one-hot sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential([\n",
    "        SimpleRNN(n_hidden, return_sequences=True, input_shape=(cs, vocab_size),\n",
    "                  activation='relu', recurrent_initializer='identity'),\n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax')),\n",
    "    ])\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75109, 8, 85), (75109, 8, 85))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "oh_ys = [to_categorical(o, vocab_size) for o in ys]\n",
    "oh_y_rnn=np.stack(oh_ys, axis=1)\n",
    "\n",
    "oh_xs = [to_categorical(o, vocab_size) for o in xs]\n",
    "oh_x_rnn=np.stack(oh_xs, axis=1)\n",
    "\n",
    "oh_x_rnn.shape, oh_y_rnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "75109/75109 [==============================] - 7s - loss: 2.4442     \n",
      "Epoch 2/8\n",
      "75109/75109 [==============================] - 7s - loss: 2.0342     \n",
      "Epoch 3/8\n",
      "75109/75109 [==============================] - 7s - loss: 1.9211     \n",
      "Epoch 4/8\n",
      "75109/75109 [==============================] - 7s - loss: 1.8568     \n",
      "Epoch 5/8\n",
      "75109/75109 [==============================] - 7s - loss: 1.8141     \n",
      "Epoch 6/8\n",
      "75109/75109 [==============================] - 7s - loss: 1.7830     \n",
      "Epoch 7/8\n",
      "75109/75109 [==============================] - 7s - loss: 1.7597     \n",
      "Epoch 8/8\n",
      "75109/75109 [==============================] - 7s - loss: 1.7414     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb3f0af8940>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(oh_x_rnn, oh_y_rnn, batch_size=64, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nexts_oh(inp):\n",
    "    idxs = np.array([char_indices[c] for c in inp])\n",
    "    arr = to_categorical(idxs, vocab_size)\n",
    "    p = model.predict(arr[np.newaxis,:])[0]\n",
    "    print(list(inp))\n",
    "    return [chars[np.argmax(o)] for o in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 't', 'h', 'i', 's', ' ', 'i', 's']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['t', 'h', 'e', 's', ' ', 'c', 's', ' ']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts_oh(' this is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pure python rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.,  0.]), array([ 1.,  0.]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def sigmoid_d(x): \n",
    "    output = sigmoid(x)\n",
    "    return output*(1-output)\n",
    "def relu(x):\n",
    "    return np.maximum(0., x)\n",
    "def relu_d(x):\n",
    "    return (x > 0.)*1.\n",
    "\n",
    "relu(np.array([3.,-3.])), relu_d(np.array([3.,-3.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(a,b):\n",
    "    return pow(a-b,2)\n",
    "def dist_d(a,b):\n",
    "    return 2*(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Enter 'c' at the ipdb>  prompt to continue execution.\n",
      "None\n",
      "> \u001b[0;32m<string>\u001b[0m(2)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\n",
      "ipdb> c\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "%%debug\n",
    "for i in range(5):\n",
    "    #pdb.set_trace()\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps = 1e-7\n",
    "def x_entropy(pred, actual): \n",
    "    return -np.sum(actual * np.log(np.clip(pred, eps, 1-eps)))\n",
    "def x_entropy_d(pred, actual):\n",
    "    return -actual/pred\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.exp(x).sum()\n",
    "def softmax_d(x):\n",
    "    sm = softmax(x)\n",
    "    res = np.expand_dims(-sm,-1)*sm\n",
    "    res[np.diag_indices_from(res)] = sm*(1-sm)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.35667494393873245, array([-0.        , -1.42857143, -0.        ]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = np.array([0.2,0.7,0.1])\n",
    "test_actuals = np.array([0.,1.,0.])\n",
    "x_entropy(test_preds, test_actuals), x_entropy_d(test_preds, test_actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.28140804,  0.46396343,  0.25462853]),\n",
       " array([[ 0.20221756, -0.13056304, -0.07165452],\n",
       "        [-0.13056304,  0.24870137, -0.11813832],\n",
       "        [-0.07165452, -0.11813832,  0.18979284]]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(test_preds), softmax_d(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 6, 10]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act=relu\n",
    "act_d = relu_d\n",
    "loss=x_entropy\n",
    "loss_d=x_entropy_d\n",
    "\n",
    "def scan(fn, start, seq):\n",
    "    res = []\n",
    "    prev = start\n",
    "    for s in seq:\n",
    "        app = fn(prev, s)\n",
    "        res.append(app)\n",
    "        prev = app\n",
    "    return res\n",
    "\n",
    "scan(lambda prev,curr: prev+curr, 0, range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75109, 8, 85), (75109, 8, 85))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "oh_ys = [to_categorical(o, vocab_size) for o in ys]\n",
    "oh_y_rnn=np.stack(oh_ys, axis=1)\n",
    "\n",
    "oh_xs = [to_categorical(o, vocab_size) for o in xs]\n",
    "oh_x_rnn=np.stack(oh_xs, axis=1)\n",
    "\n",
    "oh_x_rnn.shape, oh_y_rnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = oh_x_rnn\n",
    "outp = oh_y_rnn\n",
    "n_input = vocab_size\n",
    "n_output = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_char(prev, item):\n",
    "    # Previous state\n",
    "    tot_loss, pre_hidden, pre_pred, hidden, ypred = prev\n",
    "    # Current inputs and output\n",
    "    x, y = item\n",
    "    pre_hidden = np.dot(x,w_x) + np.dot(hidden,w_h)\n",
    "    hidden = act(pre_hidden)\n",
    "    pre_pred = np.dot(hidden,w_y)\n",
    "    ypred = softmax(pre_pred)\n",
    "    return (\n",
    "        # Keep track of loss so we can report it\n",
    "        tot_loss+loss(ypred, y),\n",
    "        # Used in backprop\n",
    "        pre_hidden, pre_pred, \n",
    "        # Used in next iteration\n",
    "        hidden, \n",
    "        # To provide predictions\n",
    "        ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_chars(n):\n",
    "    return zip(inp[n], outp[n])\n",
    "def one_fwd(n):\n",
    "    return scan(one_char, (0,0,0,np.zeros(n_hidden),0), get_chars(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"Columnify\" a vector\n",
    "def col(x): \n",
    "    return x[:,np.newaxis]\n",
    "\n",
    "def one_bkwd(args, n):\n",
    "    global w_x,w_y,w_h\n",
    "\n",
    "    i=inp[n]  # 8x86\n",
    "    o=outp[n] # 8x86\n",
    "    d_pre_hidden = np.zeros(n_hidden) # 256\n",
    "    for p in reversed(range(len(i))):\n",
    "        totloss, pre_hidden, pre_pred, hidden, ypred = args[p]\n",
    "        x=i[p] # 86\n",
    "        y=o[p] # 86\n",
    "        d_pre_pred = softmax_d(pre_pred).dot(loss_d(ypred,y))  # 86\n",
    "        d_pre_hidden = (np.dot(d_pre_hidden, w_h.T) \n",
    "                        + np.dot(d_pre_pred,w_y.T)) * act_d(pre_hidden) # 256\n",
    "\n",
    "        # d(loss)/d(w_y) = d(loss)/d(pre_pred) * d(pre_pred)/d(w_y)\n",
    "        w_y -= col(hidden) * d_pre_pred * alpha\n",
    "        # d(loss)/d(w_h) = d(loss)/d(pre_hidden[p-1]) * d(pre_hidden[p-1])/d(w_h)\n",
    "        if (p>0): w_h -= args[p-1][3].dot(d_pre_hidden) * alpha\n",
    "        w_x -= col(x)*d_pre_hidden * alpha\n",
    "    return d_pre_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "scale=math.sqrt(2./n_input)\n",
    "w_x = np.random.normal(scale=scale, size=(n_input,n_hidden))\n",
    "w_y = np.random.normal(scale=scale, size=(n_hidden, n_output))\n",
    "w_h = np.eye(n_hidden, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:35.8079; Gradient:1.73636\n",
      "Error:35.6095; Gradient:1.88083\n",
      "Error:35.4611; Gradient:1.84063\n",
      "Error:35.3487; Gradient:1.75219\n",
      "Error:35.1858; Gradient:1.77124\n",
      "Error:35.0096; Gradient:2.13609\n",
      "Error:33.2007; Gradient:4.50439\n",
      "Error:31.5541; Gradient:4.17145\n",
      "Error:30.1903; Gradient:4.69561\n",
      "Error:29.6151; Gradient:4.88102\n"
     ]
    }
   ],
   "source": [
    "overallError=0\n",
    "alpha=0.0001\n",
    "for n in range(10000):\n",
    "    res = one_fwd(n)\n",
    "    overallError+=res[-1][0]\n",
    "    deriv = one_bkwd(res, n)\n",
    "    if(n % 1000 == 999):\n",
    "        print (\"Error:{:.4f}; Gradient:{:.5f}\".format(\n",
    "                overallError/1000, np.linalg.norm(deriv)))\n",
    "        overallError=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import GRU\n",
    "\n",
    "model=Sequential([\n",
    "        GRU(n_hidden, return_sequences=True, input_shape=(cs, vocab_size),\n",
    "                  activation='relu'),\n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax')),\n",
    "    ])\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "75109/75109 [==============================] - 16s - loss: 2.4032    \n",
      "Epoch 2/8\n",
      "75109/75109 [==============================] - 16s - loss: 1.9872    \n",
      "Epoch 3/8\n",
      "75109/75109 [==============================] - 16s - loss: 1.8770    \n",
      "Epoch 4/8\n",
      "75109/75109 [==============================] - 16s - loss: 1.8143    \n",
      "Epoch 5/8\n",
      "75109/75109 [==============================] - 16s - loss: 1.7711    \n",
      "Epoch 6/8\n",
      "75109/75109 [==============================] - 16s - loss: 1.7398    \n",
      "Epoch 7/8\n",
      "75109/75109 [==============================] - 16s - loss: 1.7158    \n",
      "Epoch 8/8\n",
      "75109/75109 [==============================] - 16s - loss: 1.6955    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb3e8e4a5c0>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(oh_x_rnn, oh_y_rnn, batch_size=64, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 't', 'h', 'i', 's', ' ', 'i', 's']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['t', 'h', 'e', 's', ' ', 'i', 'n', ' ']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts_oh(' this is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
